{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Monarch Ingest The Monarch Ingest generates KGX formatted files conforming to the BioLink Model from a wide variety of biomedical data sources. Kozathon Get set up for Kozathon Getting Started First, clone the repository gh repo clone monarch-initiative/monarch-ingest && cd monarch-ingest The Monarch Ingest is built using Poetry , which will create its own virtual environment. make all This will install Poetry, create the virtual environment and fetch dependencies. Downloading data Using the Google Cloud cli you can download the whole data bucket gsutil -m cp -R gs://monarch-ingest/data . Running an ingest poetry run koza transform --global-table monarch_ingest/translation_table.yaml --source monarch_ingest/alliance/metadata.yaml --output-format tsv Validate the output poetry run kgx validate -i tsv output/Alliance.gene-to-phenotype_nodes.tsv output/Alliance.gene-to-phenotype_edges.tsv Running all transforms make transform KGX Merge To merge all of the transformed files with ontologies into a single pair of node and edge files make merge TLDR If you want to generate it all as output/merged/monarch-kg.tar.gz make download transform merge","title":"Home"},{"location":"#monarch-ingest","text":"The Monarch Ingest generates KGX formatted files conforming to the BioLink Model from a wide variety of biomedical data sources.","title":"Monarch Ingest"},{"location":"#kozathon","text":"Get set up for Kozathon","title":"Kozathon"},{"location":"#getting-started","text":"First, clone the repository gh repo clone monarch-initiative/monarch-ingest && cd monarch-ingest The Monarch Ingest is built using Poetry , which will create its own virtual environment. make all This will install Poetry, create the virtual environment and fetch dependencies.","title":"Getting Started"},{"location":"#downloading-data","text":"Using the Google Cloud cli you can download the whole data bucket gsutil -m cp -R gs://monarch-ingest/data .","title":"Downloading data"},{"location":"#running-an-ingest","text":"poetry run koza transform --global-table monarch_ingest/translation_table.yaml --source monarch_ingest/alliance/metadata.yaml --output-format tsv","title":"Running an ingest"},{"location":"#validate-the-output","text":"poetry run kgx validate -i tsv output/Alliance.gene-to-phenotype_nodes.tsv output/Alliance.gene-to-phenotype_edges.tsv","title":"Validate the output"},{"location":"#running-all-transforms","text":"make transform","title":"Running all transforms"},{"location":"#kgx-merge","text":"To merge all of the transformed files with ontologies into a single pair of node and edge files make merge","title":"KGX Merge"},{"location":"#tldr","text":"If you want to generate it all as output/merged/monarch-kg.tar.gz make download transform merge","title":"TLDR"},{"location":"ingests/alliance/","text":"The Alliance of Genome Resources contains a subset of model organism data from member databases that is harmonized to the same model. Over time, as the alliance adds additional data types, individual MOD ingests can be replaced by collective Alliance ingest. The Alliance has bulk data downloads, ingest data formats, and an API. The preference should be bulk downloads first, followed by ingest formats, finally by API calls. In some cases it may continue to be more practical to load from individual MODs when data is not yet fully harmonized in the Alliance. Alliance Bulk Downloads Alliance schemas Gene Information Genes for all Alliance species (Human, Rat, Mouse, Fish, Fly, Worm, Yeast) are loaded using the BGI formatted ingest files, as there are no Gene export files. Biolink captured biolink:Gene id symbol name type (Sequence Ontology term ID) in_taxon source synonyms xrefs Gene to Phenotype Phenotype for the subset of Alliance species which use phenotype ontologies (Human, Rat, Mouse, Worm) are loaded using the phenotype ingest format , since there is not yet a phenotype export file from the Alliance. This file contains both Gene and Allele phenotypes, so a single column TSV is produced from BGI files listing Gene IDs to check the category and only genes are included. Environmental conditions are present for some species and are captured using the qualifier. Biolink captured biolink:Gene id biolink:PhenotypicFeature id biolink:GeneToPhenotypicFeatureAssociation id (random uuid) subject (gene.id) predicate (has_phenotype) object (phenotypicFeature.id) relation (has phenotype) publications qualifiers (condition terms) Gene to Disease Alliance disease associations Notes: including only genes excluding any experimental conditions to start, since they're just text descriptions rather than terms. (could exclude only rows that have 'Induced By' prefixing the conditions description?) Still needs to be updated to handle the ECO terms when supported by the biolink model. Need a predicate for each kind of relationship: Alliance AssociationType predicate relation biomarker_via_orthology biolionk:biomarker_for currently excluded, is this is_marker_for, but with a qualifier? implicated_via_orthology biolink:contributes_to currently excluded, is this is_implicated_in, but with a qualifier? is_implicated_in biolink:contributes_to \"RO:0003302\" is_marker_for biolink:biomarker_for \"RO:0002607\" is_model_of biolink:model_of \"RO:0003301\" is_not_implicated_in biolink:contributes_to + negated=true \"RO:0003302\" Biolink captured biolink:Gene id (row['DBObjectID']) biolink:Disease id (row['DOID']) biolink:GeneToDiseaseAssociation id (random uuid) subject (gene.id) predicates (see table above) object (disease.id) negated (for 'is not implicated in') relation (see predicate table above? ) publications (row['Reference']) source (row['source'])","title":"Alliance of Genome Resources"},{"location":"ingests/alliance/#gene-information","text":"Genes for all Alliance species (Human, Rat, Mouse, Fish, Fly, Worm, Yeast) are loaded using the BGI formatted ingest files, as there are no Gene export files.","title":"Gene Information"},{"location":"ingests/alliance/#biolink-captured","text":"biolink:Gene id symbol name type (Sequence Ontology term ID) in_taxon source synonyms xrefs","title":"Biolink captured"},{"location":"ingests/alliance/#gene-to-phenotype","text":"Phenotype for the subset of Alliance species which use phenotype ontologies (Human, Rat, Mouse, Worm) are loaded using the phenotype ingest format , since there is not yet a phenotype export file from the Alliance. This file contains both Gene and Allele phenotypes, so a single column TSV is produced from BGI files listing Gene IDs to check the category and only genes are included. Environmental conditions are present for some species and are captured using the qualifier.","title":"Gene to Phenotype"},{"location":"ingests/alliance/#biolink-captured_1","text":"biolink:Gene id biolink:PhenotypicFeature id biolink:GeneToPhenotypicFeatureAssociation id (random uuid) subject (gene.id) predicate (has_phenotype) object (phenotypicFeature.id) relation (has phenotype) publications qualifiers (condition terms)","title":"Biolink captured"},{"location":"ingests/alliance/#gene-to-disease","text":"Alliance disease associations Notes: including only genes excluding any experimental conditions to start, since they're just text descriptions rather than terms. (could exclude only rows that have 'Induced By' prefixing the conditions description?) Still needs to be updated to handle the ECO terms when supported by the biolink model. Need a predicate for each kind of relationship: Alliance AssociationType predicate relation biomarker_via_orthology biolionk:biomarker_for currently excluded, is this is_marker_for, but with a qualifier? implicated_via_orthology biolink:contributes_to currently excluded, is this is_implicated_in, but with a qualifier? is_implicated_in biolink:contributes_to \"RO:0003302\" is_marker_for biolink:biomarker_for \"RO:0002607\" is_model_of biolink:model_of \"RO:0003301\" is_not_implicated_in biolink:contributes_to + negated=true \"RO:0003302\"","title":"Gene to Disease"},{"location":"ingests/alliance/#biolink-captured_2","text":"biolink:Gene id (row['DBObjectID']) biolink:Disease id (row['DOID']) biolink:GeneToDiseaseAssociation id (random uuid) subject (gene.id) predicates (see table above) object (disease.id) negated (for 'is not implicated in') relation (see predicate table above? ) publications (row['Reference']) source (row['source'])","title":"Biolink captured"},{"location":"ingests/pombase/","text":"PomBase is a comprehensive database for the fission yeast Schizosaccharomyces pombe, providing structural and functional annotation, literature curation and access to large-scale data sets. Within this ingest there will be a transformation of gene to phenotypic feature associations, gene entities aren't yet loaded as a part of this ingest, and FYPO ontology terms will be brought in directly from the ontology without transformation. PomBase Bulk Downloads Phaf Format Description Phaf Format LinkML Gene to Phenotype The PHAF download file is extremely well documented. Alleles provided, but not captured, with the assumption that even with an allele specified the gene to phenotype is accurate with a some-some interpretation. Genotype/strain information looks uniform throughout the file, and is not captured. It might be sensible to make presence of genotype information an error condition to be sure that we only get 'clean' gene to phenotype associations. Penetrance and Severity columns are available, but not captured as a part of this ingest. Penetrance values can be either FYPO_EXT terms (FYPO_EXT:0000001, FYPO_EXT:0000002, FYPO_EXT:0000003, FYPO_EXT:0000004), int/float numbers (percentages), or strings (\">98\", \"~10\", \"10-20\"). Severity is represented using one or more FYPO_EXT terms. Biolink captured biolink:Gene id biolink:PhenotypicFeature id biolink:GeneToPhenotypicFeatureAssociation id (random uuid) subject (gene.id) predicate (has_phenotype) object (phenotypicFeature.id) relation (has phenotype) publications qualifers (optionally included from condition row)","title":"PomBase"},{"location":"ingests/pombase/#gene-to-phenotype","text":"The PHAF download file is extremely well documented. Alleles provided, but not captured, with the assumption that even with an allele specified the gene to phenotype is accurate with a some-some interpretation. Genotype/strain information looks uniform throughout the file, and is not captured. It might be sensible to make presence of genotype information an error condition to be sure that we only get 'clean' gene to phenotype associations. Penetrance and Severity columns are available, but not captured as a part of this ingest. Penetrance values can be either FYPO_EXT terms (FYPO_EXT:0000001, FYPO_EXT:0000002, FYPO_EXT:0000003, FYPO_EXT:0000004), int/float numbers (percentages), or strings (\">98\", \"~10\", \"10-20\"). Severity is represented using one or more FYPO_EXT terms.","title":"Gene to Phenotype"},{"location":"ingests/pombase/#biolink-captured","text":"biolink:Gene id biolink:PhenotypicFeature id biolink:GeneToPhenotypicFeatureAssociation id (random uuid) subject (gene.id) predicate (has_phenotype) object (phenotypicFeature.id) relation (has phenotype) publications qualifers (optionally included from condition row)","title":"Biolink captured"},{"location":"ingests/xenbase/","text":"Xenbase is a web-accessible resource that integrates all the diverse biological, genomic, genotype and phenotype data available from Xenopus research. Xenbase Bulk Data Xenbase FTP Gene Information Xenbase genes are ingested using Koza's built in support for the GPI format rather than using the gene information file available from the bulk download. The GPI file is downloaded from http://ftp.xenbase.org/pub/GenePageReports/xenbase.gpi.gz Biolink captured Gene id symbol name synonym in_taxon xref source Gene to Phenotype This ingest is built against a one-off OBAN formatted file, which makes for a transformation which only requries adding a curie prefix and connecting column names to biolink attributes. Evidence codes are provided as ECO terms but not yet captured in the output. Biolink captured biolink:Gene id biolink:PhenotypicFeature id biolink:GeneToPhenotypicFeatureAssociation id (random uuid) subject (gene.id) predicate (has_phenotype) object (phenotypicFeature.id) relation (has phenotype) publication Gene Literature This ingest reads from Xenbase's Genes Associated with Literature file to capture associations between Xenbase's XB-GENEPAGE ids and PMIDs, then relies on a map built from Xenbase's GenepageToGeneId file to create associations from XB-GENE records to PMID records. Biolink captured Gene id Publication id NamedThingToInformationContentEntityAssociation id (random uuid) subject (gene.id) predicate (mentions) object (publication.id) relation (mentions)","title":"XenBase"},{"location":"ingests/xenbase/#gene-information","text":"Xenbase genes are ingested using Koza's built in support for the GPI format rather than using the gene information file available from the bulk download. The GPI file is downloaded from http://ftp.xenbase.org/pub/GenePageReports/xenbase.gpi.gz","title":"Gene Information"},{"location":"ingests/xenbase/#biolink-captured","text":"Gene id symbol name synonym in_taxon xref source","title":"Biolink captured"},{"location":"ingests/xenbase/#gene-to-phenotype","text":"This ingest is built against a one-off OBAN formatted file, which makes for a transformation which only requries adding a curie prefix and connecting column names to biolink attributes. Evidence codes are provided as ECO terms but not yet captured in the output.","title":"Gene to Phenotype"},{"location":"ingests/xenbase/#biolink-captured_1","text":"biolink:Gene id biolink:PhenotypicFeature id biolink:GeneToPhenotypicFeatureAssociation id (random uuid) subject (gene.id) predicate (has_phenotype) object (phenotypicFeature.id) relation (has phenotype) publication","title":"Biolink captured"},{"location":"ingests/xenbase/#gene-literature","text":"This ingest reads from Xenbase's Genes Associated with Literature file to capture associations between Xenbase's XB-GENEPAGE ids and PMIDs, then relies on a map built from Xenbase's GenepageToGeneId file to create associations from XB-GENE records to PMID records.","title":"Gene Literature"},{"location":"ingests/xenbase/#biolink-captured_2","text":"Gene id Publication id NamedThingToInformationContentEntityAssociation id (random uuid) subject (gene.id) predicate (mentions) object (publication.id) relation (mentions)","title":"Biolink captured"},{"location":"ingests/zfin/","text":"ZFIN is the Zebrafish Model Organism Database. ZFIN bulk downloads Gene to Phenotype This ingest uses ZFIN's clean gene phenotype download file, which only contains phenotypes which can safely be associated to a single affected gene. This ingest is distinct from the Alliance phenotype index because ZFIN builds Entity-Quality-Entity phenotype statements that can be built from post-composed terms (E1a+E1b+Q+E2a+E2b), Biolink captured biolink:Gene id biolink:PhenotypicFeature id biolink:GeneToPhenotypicFeatureAssociation id (random uuid) subject (gene.id) predicate (has_phenotype) object (phenotypicFeature.id) relation (has phenotype) publication Gene to Publication This ingest uses ZFIN's gene to publication download file, which only contains assocations between publications and genes that are denoted in some way in the publication. We have selected to use a consistent high level term for 'publication' (IAO:0000311) as it is heterogeneous mix of publication types being referenced. We have also opted to use the ZDB-ID for the publication node rather than a pubmed ID, on the assumption that kgx will clique merge them later. Biolink captured biolink:Gene id biolink:Publication id biolink:NamedThingToInformationContentEntityAssociation id (random uuid) subject (gene.id) predicate (mentions) object (publication.id) relation (mentions)","title":"ZFIN"},{"location":"ingests/zfin/#gene-to-phenotype","text":"This ingest uses ZFIN's clean gene phenotype download file, which only contains phenotypes which can safely be associated to a single affected gene. This ingest is distinct from the Alliance phenotype index because ZFIN builds Entity-Quality-Entity phenotype statements that can be built from post-composed terms (E1a+E1b+Q+E2a+E2b),","title":"Gene to Phenotype"},{"location":"ingests/zfin/#biolink-captured","text":"biolink:Gene id biolink:PhenotypicFeature id biolink:GeneToPhenotypicFeatureAssociation id (random uuid) subject (gene.id) predicate (has_phenotype) object (phenotypicFeature.id) relation (has phenotype) publication","title":"Biolink captured"},{"location":"ingests/zfin/#gene-to-publication","text":"This ingest uses ZFIN's gene to publication download file, which only contains assocations between publications and genes that are denoted in some way in the publication. We have selected to use a consistent high level term for 'publication' (IAO:0000311) as it is heterogeneous mix of publication types being referenced. We have also opted to use the ZDB-ID for the publication node rather than a pubmed ID, on the assumption that kgx will clique merge them later.","title":"Gene to Publication"},{"location":"ingests/zfin/#biolink-captured_1","text":"biolink:Gene id biolink:Publication id biolink:NamedThingToInformationContentEntityAssociation id (random uuid) subject (gene.id) predicate (mentions) object (publication.id) relation (mentions)","title":"Biolink captured"},{"location":"kozathon/setup/","text":"Kozathon Setup First, clone the repository git clone https://github.com/monarch-initiative/monarch-ingest.git && cd monarch-ingest The Monarch Ingest is built using Poetry , which will create its own virtual environment. make all This will install Poetry, create the virtual environment and fetch dependencies. Downloading data Download data files for our demo ingests. PomBase's phenotype annotations, a mini-StringDB file, and the HPOA file. #Assuming you are still in monarch-ingest dir mkdir -p data cd data curl -OJ https://www.pombase.org/data/annotations/Phenotype_annotations/phenotype_annotations.pombase.phaf.gz curl -OJ https://raw.githubusercontent.com/monarch-initiative/koza/main/tests/resources/source-files/string.tsv curl -OJ http://purl.obolibrary.org/obo/hp/hpoa/phenotype.hpoa cd .. Make sure the PomBase ingest is working poetry run koza transform --global-table monarch_ingest/translation_table.yaml --source monarch_ingest/pombase/metadata.yaml --output-format tsv Validate the output* poetry run kgx validate -i tsv output/PomBase.gene-to-phenotype_nodes.tsv output/PomBase.gene-to-phenotype_edges.tsv \ud83d\udc4d You're ready for Kozathon In the meantime, check out the tutorial","title":"Setup"},{"location":"kozathon/setup/#kozathon","text":"","title":"Kozathon"},{"location":"kozathon/setup/#setup","text":"First, clone the repository git clone https://github.com/monarch-initiative/monarch-ingest.git && cd monarch-ingest The Monarch Ingest is built using Poetry , which will create its own virtual environment. make all This will install Poetry, create the virtual environment and fetch dependencies.","title":"Setup"},{"location":"kozathon/setup/#downloading-data","text":"Download data files for our demo ingests. PomBase's phenotype annotations, a mini-StringDB file, and the HPOA file. #Assuming you are still in monarch-ingest dir mkdir -p data cd data curl -OJ https://www.pombase.org/data/annotations/Phenotype_annotations/phenotype_annotations.pombase.phaf.gz curl -OJ https://raw.githubusercontent.com/monarch-initiative/koza/main/tests/resources/source-files/string.tsv curl -OJ http://purl.obolibrary.org/obo/hp/hpoa/phenotype.hpoa cd ..","title":"Downloading data"},{"location":"kozathon/setup/#make-sure-the-pombase-ingest-is-working","text":"poetry run koza transform --global-table monarch_ingest/translation_table.yaml --source monarch_ingest/pombase/metadata.yaml --output-format tsv","title":"Make sure the PomBase ingest is working"},{"location":"kozathon/setup/#validate-the-output","text":"poetry run kgx validate -i tsv output/PomBase.gene-to-phenotype_nodes.tsv output/PomBase.gene-to-phenotype_edges.tsv","title":"Validate the output*"},{"location":"kozathon/setup/#youre-ready-for-kozathon","text":"In the meantime, check out the tutorial","title":"\ud83d\udc4d You're ready for Kozathon"},{"location":"kozathon%202/setup/","text":"Kozathon Setup First, clone the repository git clone https://github.com/monarch-initiative/monarch-ingest.git && cd monarch-ingest The Monarch Ingest is built using Poetry , which will create its own virtual environment. make all This will install Poetry, create the virtual environment and fetch dependencies. Downloading data Download data files for our demo ingests. PomBase's phenotype annotations, a mini-StringDB file, and the HPOA file. #Assuming you are still in monarch-ingest dir mkdir -p data cd data curl -OJ https://www.pombase.org/data/annotations/Phenotype_annotations/phenotype_annotations.pombase.phaf.gz curl -OJ https://raw.githubusercontent.com/monarch-initiative/koza/main/tests/resources/source-files/string.tsv curl -OJ http://purl.obolibrary.org/obo/hp/hpoa/phenotype.hpoa cd .. Make sure the PomBase ingest is working poetry run koza transform --global-table monarch_ingest/translation_table.yaml --source monarch_ingest/pombase/metadata.yaml --output-format tsv Validate the output* poetry run kgx validate -i tsv output/PomBase.gene-to-phenotype_nodes.tsv output/PomBase.gene-to-phenotype_edges.tsv \ud83d\udc4d You're ready for Kozathon In the meantime, check out the tutorial","title":"Setup"},{"location":"kozathon%202/setup/#kozathon","text":"","title":"Kozathon"},{"location":"kozathon%202/setup/#setup","text":"First, clone the repository git clone https://github.com/monarch-initiative/monarch-ingest.git && cd monarch-ingest The Monarch Ingest is built using Poetry , which will create its own virtual environment. make all This will install Poetry, create the virtual environment and fetch dependencies.","title":"Setup"},{"location":"kozathon%202/setup/#downloading-data","text":"Download data files for our demo ingests. PomBase's phenotype annotations, a mini-StringDB file, and the HPOA file. #Assuming you are still in monarch-ingest dir mkdir -p data cd data curl -OJ https://www.pombase.org/data/annotations/Phenotype_annotations/phenotype_annotations.pombase.phaf.gz curl -OJ https://raw.githubusercontent.com/monarch-initiative/koza/main/tests/resources/source-files/string.tsv curl -OJ http://purl.obolibrary.org/obo/hp/hpoa/phenotype.hpoa cd ..","title":"Downloading data"},{"location":"kozathon%202/setup/#make-sure-the-pombase-ingest-is-working","text":"poetry run koza transform --global-table monarch_ingest/translation_table.yaml --source monarch_ingest/pombase/metadata.yaml --output-format tsv","title":"Make sure the PomBase ingest is working"},{"location":"kozathon%202/setup/#validate-the-output","text":"poetry run kgx validate -i tsv output/PomBase.gene-to-phenotype_nodes.tsv output/PomBase.gene-to-phenotype_edges.tsv","title":"Validate the output*"},{"location":"kozathon%202/setup/#youre-ready-for-kozathon","text":"In the meantime, check out the tutorial","title":"\ud83d\udc4d You're ready for Kozathon"},{"location":"tutorials/add-documentation/","text":"Document Ingest The documentation for an ingest should reflect both the decision-making process that led to the output, and the output itself. Begin by copying the source.md file to the documentation folder, renaming it to match the ingest name and adding it to mkdocs.yaml in the root. This is a great time to look over the columns in the ingest file and consider what biolink classes are appropriate to represent them and what fields are available to populate on each. Some helpful resources: * Biolink Documentation * List of Biolink Associations * Use a Jupyter Notebook with Biolink Model Toolkit to do things like get_element_by_mapping('RO:0002410') * For ingests migrating from Dipper, check out the documentation and source code Next Begin implementation","title":"Document"},{"location":"tutorials/add-documentation/#document-ingest","text":"The documentation for an ingest should reflect both the decision-making process that led to the output, and the output itself. Begin by copying the source.md file to the documentation folder, renaming it to match the ingest name and adding it to mkdocs.yaml in the root. This is a great time to look over the columns in the ingest file and consider what biolink classes are appropriate to represent them and what fields are available to populate on each. Some helpful resources: * Biolink Documentation * List of Biolink Associations * Use a Jupyter Notebook with Biolink Model Toolkit to do things like get_element_by_mapping('RO:0002410') * For ingests migrating from Dipper, check out the documentation and source code","title":"Document Ingest"},{"location":"tutorials/add-documentation/#next","text":"Begin implementation","title":"Next"},{"location":"tutorials/configure-ingest/","text":"Configure Ingest download the file and put it in the data directory (This is the process until we have local downloads) Create the directory mkdir monarch_ingest/somethingbase Copy the template cp source_template/* monarch_ingest/somethingbase Edit metadata.yaml Update the description, rights link, url, etc and then add your source_file Edit the source file yaml match the columns or required fields up with what's available in the file to be ingested If it's an ingest that exists in Dipper , check out what Dipper does. Check the Biolink Model documation to look at what you can capture If what we need from an ingest can't be captured in the model yet, make a new Biolink issue Set the header properties If there is no header at all, set header: False If there are comment lines before the header, count them and set skip_lines: {n} Next Begin documentation","title":"Configure"},{"location":"tutorials/configure-ingest/#configure-ingest","text":"download the file and put it in the data directory (This is the process until we have local downloads)","title":"Configure Ingest"},{"location":"tutorials/configure-ingest/#create-the-directory","text":"mkdir monarch_ingest/somethingbase","title":"Create the directory"},{"location":"tutorials/configure-ingest/#copy-the-template","text":"cp source_template/* monarch_ingest/somethingbase","title":"Copy the template"},{"location":"tutorials/configure-ingest/#edit-metadatayaml","text":"Update the description, rights link, url, etc and then add your source_file","title":"Edit metadata.yaml"},{"location":"tutorials/configure-ingest/#edit-the-source-file-yaml","text":"match the columns or required fields up with what's available in the file to be ingested If it's an ingest that exists in Dipper , check out what Dipper does. Check the Biolink Model documation to look at what you can capture If what we need from an ingest can't be captured in the model yet, make a new Biolink issue Set the header properties If there is no header at all, set header: False If there are comment lines before the header, count them and set skip_lines: {n}","title":"Edit the source file yaml"},{"location":"tutorials/configure-ingest/#next","text":"Begin documentation","title":"Next"},{"location":"tutorials/implement-ingest/","text":"Implement Ingest Most Koza scripts can run in flat mode, which means that the transform code itself doesn't need to handle the looping mechanism, and instead the transform code will have a row injected at the top and call the write command at the bottom. In between fields from the incoming row should be mapped to Biolink instances. Imports and setup Start with the imports, and make sure to set the source_name, which will be used for communicating with the reader and writer. from biolink_model_pydantic.model import Gene # The source name is used for reading and writing source_name = \"gene-information\" Inject the row # inject a single row from the source row = koza_app.get_row(source_name) Extras Next up handle any additional set up for the ingest, such as including a map or bringing in the CURIE cleaning service curie_cleaner = koza_app.curie_cleaner eqe2zp = koza_app.get_map(\"eqe2zp\") translation_table = koza_app.translation_table Creating entities At this step, hopefully your documentation is so good that you're just letting your fingers take on the last step of converting what you've already planned into Python syntax. Ideally not much logic will be needed here, and if there's a lot, it might be worth considering whether an ingest (even on the same file) can be split across multiple transforms so that each is as easy to read as possible. Aim to add all properties when creating the instance, but in some cases adding optional lists might need to happen below. gene = Gene( id='somethingbase:'+row['ID'], name=row['Name'] ) # populate any additional optional properties if row['xrefs']: gene.xrefs = [curie_cleaner.clean(xref) for xref in row['xrefs']] Writing At the end of the script, call the writer. The first argument must be the source_name (so that it will know where to write), entities should be passed in as additional arguments. koza_app.write(gene, phenotypicFeature, association) Running your ingest To execute the ingest from outside of a poetry shell: poetry run koza transform --global-table monarch_ingest/translation_table.yaml --source monarch_ingest/<YOUR SOURCE>/<YOUR INGEST>.yaml Next Testing! This is also a good time to circle back and update the documentation.","title":"Implement"},{"location":"tutorials/implement-ingest/#implement-ingest","text":"Most Koza scripts can run in flat mode, which means that the transform code itself doesn't need to handle the looping mechanism, and instead the transform code will have a row injected at the top and call the write command at the bottom. In between fields from the incoming row should be mapped to Biolink instances.","title":"Implement Ingest"},{"location":"tutorials/implement-ingest/#imports-and-setup","text":"Start with the imports, and make sure to set the source_name, which will be used for communicating with the reader and writer. from biolink_model_pydantic.model import Gene # The source name is used for reading and writing source_name = \"gene-information\"","title":"Imports and setup"},{"location":"tutorials/implement-ingest/#inject-the-row","text":"# inject a single row from the source row = koza_app.get_row(source_name)","title":"Inject the row"},{"location":"tutorials/implement-ingest/#extras","text":"Next up handle any additional set up for the ingest, such as including a map or bringing in the CURIE cleaning service curie_cleaner = koza_app.curie_cleaner eqe2zp = koza_app.get_map(\"eqe2zp\") translation_table = koza_app.translation_table","title":"Extras"},{"location":"tutorials/implement-ingest/#creating-entities","text":"At this step, hopefully your documentation is so good that you're just letting your fingers take on the last step of converting what you've already planned into Python syntax. Ideally not much logic will be needed here, and if there's a lot, it might be worth considering whether an ingest (even on the same file) can be split across multiple transforms so that each is as easy to read as possible. Aim to add all properties when creating the instance, but in some cases adding optional lists might need to happen below. gene = Gene( id='somethingbase:'+row['ID'], name=row['Name'] ) # populate any additional optional properties if row['xrefs']: gene.xrefs = [curie_cleaner.clean(xref) for xref in row['xrefs']]","title":"Creating entities"},{"location":"tutorials/implement-ingest/#writing","text":"At the end of the script, call the writer. The first argument must be the source_name (so that it will know where to write), entities should be passed in as additional arguments. koza_app.write(gene, phenotypicFeature, association)","title":"Writing"},{"location":"tutorials/implement-ingest/#running-your-ingest","text":"To execute the ingest from outside of a poetry shell: poetry run koza transform --global-table monarch_ingest/translation_table.yaml --source monarch_ingest/<YOUR SOURCE>/<YOUR INGEST>.yaml","title":"Running your ingest"},{"location":"tutorials/implement-ingest/#next","text":"Testing! This is also a good time to circle back and update the documentation.","title":"Next"},{"location":"tutorials/test-ingest/","text":"Testing You may want to start with the test template within source_template Basic fixtures Initially, set up your basic fixtures, taking care to set the correct source name and location for the transform code. import pytest from koza.koza_runner import get_translation_table @pytest.fixture def tt(): return get_translation_table(\"monarch_ingest/translation_table.yaml\", None) # This name must match the ingest name in the transform code @pytest.fixture def source_name(): return \"something-to-somethingelse\" # This is the location of the transform code @pytest.fixture def script(): return \"./monarch_ingest/somethingbase/something2somethingelse.py\" A map, if necessary Some ingests will depend on one or more maps, that fixture can be set up here. Note that this fixture must return a map of maps, and that the inner maps will map from an ID to a dictionary representing column headers and values. In the example below, a map is created that maps from a big concatenated natural key (as the ID) for ZP to a single column (called iri ) that contains the ZP ID. This map is then placed into the map cache under the name eqe2zp @pytest.fixture def map_cache(): eqe2zp = { \"0-0-ZFA:0000042-PATO:0000638-0-0-0\": {\"iri\": \"ZP:0004225\"}, \"BSPO:0000112-BFO:0000050-ZFA:0000042-PATO:0000638-0-0-0\": { \"iri\": \"ZP:0011243\" }, \"BSPO:0000000-BFO:0000050-ZFA:0000823-PATO:0000642-BSPO:0000007-BFO:0000050-ZFA:0000823\": { \"iri\": \"ZP:0000157\" }, } return {\"eqe2zp\": eqe2zp} Fixtures for test data Create a fixture that returns a dictionary to represent a single row. As a matter of strategy, this row should probably represent a fairly basic row being ingested. One trick so that you don't have to manually convert from the imput format to a python dictionary format is to run your ingest with a debugger and set a breakpoint just after a row has been injected. If you want a more specific piece of data, check out conditional breakpoints. @pytest.fixture def basic_row(): return { \"ID\": \"341492416\", \"Gene Symbol\": \"pax2a\", \"Gene ID\": \"ZDB-GENE-990415-8\", #... \"Fish Environment ID\": \"ZDB-GENOX-041102-1385\", \"Publication ID\": \"ZDB-PUB-970210-19\", \"Figure ID\": \"ZDB-FIG-120307-8\", } Fixture for transforming a single row This sets up a fixture you can call more than once to independently test different attributes @pytest.fixture def basic_g2p(mock_koza, source_name, basic_row, script, map_cache, tt): return mock_koza( source_name, iter([basic_row]), script, map_cache=map_cache, translation_table=tt, ) Test the basics of the ingest Confirm that entities are created matching the expectations on the row # A simple end-to-end test is to confirm that the IDs are set on def test_gene(basic_g2p): gene = basic_g2p[0] assert gene assert gene.id == \"ZFIN:ZDB-GENE-990415-8\" def test_phenotypic_feature(basic_g2p): phenotypic_feature = basic_g2p[1] assert phenotypic_feature assert phenotypic_feature.id == \"ZP:0004225\" def test_association(basic_g2p): association = basic_g2p[2] assert association assert association.subject == \"ZFIN:ZDB-GENE-990415-8\" assert association.object == \"ZP:0004225\" assert association.publications assert association.publications[0] == \"ZFIN:ZDB-PUB-970210-19\" Test against an alternate row For any branching within the transform code, it's a good idea to test against all of the paths through the code. It's possible to set conditional breakpoints to find real examples in the code that will hit each code path, but it may be more practical to modify the basic row as a new fixture The example below creates a row with additional columns filled in. @pytest.fixture def postcomposed(mock_koza, source_name, basic_row, script, map_cache, tt): basic_row[\"Affected Structure or Process 1 subterm ID\"] = \"BSPO:0000112\" basic_row[\"Post-composed Relationship ID\"] = \"BFO:0000050\" basic_row[\"Affected Structure or Process 1 superterm ID\"] = \"ZFA:0000042\" return mock_koza( source_name, iter([basic_row]), script, map_cache=map_cache, translation_table=tt, ) Parameterized tests Mixing parameterization and fixtures changes the approach a little. In this case it makes more sense to alter the row using a parameter and then create the entities within the same method. The test below is intended to confirm that when the tag column has any of the specified values, the row will be ignored (confirmed because no entities are created). @pytest.mark.parametrize(\"tag\", [\"normal\", \"exacerbated\", \"ameliorated\"]) def test_excluded_tags(mock_koza, source_name, basic_row, script, map_cache, tt, tag): basic_row[\"Phenotype Tag\"] = tag entities = mock_koza( source_name, iter([basic_row]), script, map_cache=map_cache, translation_table=tt, ) assert len(entities) == 0 Next Validate the ingest This is also a good time to circle back and update the documentation.","title":"Test"},{"location":"tutorials/test-ingest/#testing","text":"You may want to start with the test template within source_template","title":"Testing"},{"location":"tutorials/test-ingest/#basic-fixtures","text":"Initially, set up your basic fixtures, taking care to set the correct source name and location for the transform code. import pytest from koza.koza_runner import get_translation_table @pytest.fixture def tt(): return get_translation_table(\"monarch_ingest/translation_table.yaml\", None) # This name must match the ingest name in the transform code @pytest.fixture def source_name(): return \"something-to-somethingelse\" # This is the location of the transform code @pytest.fixture def script(): return \"./monarch_ingest/somethingbase/something2somethingelse.py\"","title":"Basic fixtures"},{"location":"tutorials/test-ingest/#a-map-if-necessary","text":"Some ingests will depend on one or more maps, that fixture can be set up here. Note that this fixture must return a map of maps, and that the inner maps will map from an ID to a dictionary representing column headers and values. In the example below, a map is created that maps from a big concatenated natural key (as the ID) for ZP to a single column (called iri ) that contains the ZP ID. This map is then placed into the map cache under the name eqe2zp @pytest.fixture def map_cache(): eqe2zp = { \"0-0-ZFA:0000042-PATO:0000638-0-0-0\": {\"iri\": \"ZP:0004225\"}, \"BSPO:0000112-BFO:0000050-ZFA:0000042-PATO:0000638-0-0-0\": { \"iri\": \"ZP:0011243\" }, \"BSPO:0000000-BFO:0000050-ZFA:0000823-PATO:0000642-BSPO:0000007-BFO:0000050-ZFA:0000823\": { \"iri\": \"ZP:0000157\" }, } return {\"eqe2zp\": eqe2zp}","title":"A map, if necessary"},{"location":"tutorials/test-ingest/#fixtures-for-test-data","text":"Create a fixture that returns a dictionary to represent a single row. As a matter of strategy, this row should probably represent a fairly basic row being ingested. One trick so that you don't have to manually convert from the imput format to a python dictionary format is to run your ingest with a debugger and set a breakpoint just after a row has been injected. If you want a more specific piece of data, check out conditional breakpoints. @pytest.fixture def basic_row(): return { \"ID\": \"341492416\", \"Gene Symbol\": \"pax2a\", \"Gene ID\": \"ZDB-GENE-990415-8\", #... \"Fish Environment ID\": \"ZDB-GENOX-041102-1385\", \"Publication ID\": \"ZDB-PUB-970210-19\", \"Figure ID\": \"ZDB-FIG-120307-8\", }","title":"Fixtures for test data"},{"location":"tutorials/test-ingest/#fixture-for-transforming-a-single-row","text":"This sets up a fixture you can call more than once to independently test different attributes @pytest.fixture def basic_g2p(mock_koza, source_name, basic_row, script, map_cache, tt): return mock_koza( source_name, iter([basic_row]), script, map_cache=map_cache, translation_table=tt, )","title":"Fixture for transforming a single row"},{"location":"tutorials/test-ingest/#test-the-basics-of-the-ingest","text":"Confirm that entities are created matching the expectations on the row # A simple end-to-end test is to confirm that the IDs are set on def test_gene(basic_g2p): gene = basic_g2p[0] assert gene assert gene.id == \"ZFIN:ZDB-GENE-990415-8\" def test_phenotypic_feature(basic_g2p): phenotypic_feature = basic_g2p[1] assert phenotypic_feature assert phenotypic_feature.id == \"ZP:0004225\" def test_association(basic_g2p): association = basic_g2p[2] assert association assert association.subject == \"ZFIN:ZDB-GENE-990415-8\" assert association.object == \"ZP:0004225\" assert association.publications assert association.publications[0] == \"ZFIN:ZDB-PUB-970210-19\"","title":"Test the basics of the ingest"},{"location":"tutorials/test-ingest/#test-against-an-alternate-row","text":"For any branching within the transform code, it's a good idea to test against all of the paths through the code. It's possible to set conditional breakpoints to find real examples in the code that will hit each code path, but it may be more practical to modify the basic row as a new fixture The example below creates a row with additional columns filled in. @pytest.fixture def postcomposed(mock_koza, source_name, basic_row, script, map_cache, tt): basic_row[\"Affected Structure or Process 1 subterm ID\"] = \"BSPO:0000112\" basic_row[\"Post-composed Relationship ID\"] = \"BFO:0000050\" basic_row[\"Affected Structure or Process 1 superterm ID\"] = \"ZFA:0000042\" return mock_koza( source_name, iter([basic_row]), script, map_cache=map_cache, translation_table=tt, )","title":"Test against an alternate row"},{"location":"tutorials/test-ingest/#parameterized-tests","text":"Mixing parameterization and fixtures changes the approach a little. In this case it makes more sense to alter the row using a parameter and then create the entities within the same method. The test below is intended to confirm that when the tag column has any of the specified values, the row will be ignored (confirmed because no entities are created). @pytest.mark.parametrize(\"tag\", [\"normal\", \"exacerbated\", \"ameliorated\"]) def test_excluded_tags(mock_koza, source_name, basic_row, script, map_cache, tt, tag): basic_row[\"Phenotype Tag\"] = tag entities = mock_koza( source_name, iter([basic_row]), script, map_cache=map_cache, translation_table=tt, ) assert len(entities) == 0","title":"Parameterized tests"},{"location":"tutorials/test-ingest/#next","text":"Validate the ingest This is also a good time to circle back and update the documentation.","title":"Next"},{"location":"tutorials/validate-output/","text":"KGX: Validate and add to merge config Run the transform to produce KGX files poetry run koza transform --global-table monarch_ingest/translation_table.yaml --source monarch_ingest/alliance/gene2phenotype.yaml ``` Validate the output with kgx ```bash poetry run kgx validate -i tsv output/Somethingbase.gene-to-disease_nodes.tsv output/Somethingbase.gene-to-disease_edges.tsv Finally, add the node and edge files to merge.yaml at the root of the project Next Maybe double check that documentation one last time.","title":"Validate"},{"location":"tutorials/validate-output/#kgx-validate-and-add-to-merge-config","text":"Run the transform to produce KGX files poetry run koza transform --global-table monarch_ingest/translation_table.yaml --source monarch_ingest/alliance/gene2phenotype.yaml ``` Validate the output with kgx ```bash poetry run kgx validate -i tsv output/Somethingbase.gene-to-disease_nodes.tsv output/Somethingbase.gene-to-disease_edges.tsv Finally, add the node and edge files to merge.yaml at the root of the project","title":"KGX: Validate and add to merge config"},{"location":"tutorials/validate-output/#next","text":"Maybe double check that documentation one last time.","title":"Next"}]}